# Example paper: Sauro & Lewis (2011)

## Presentation

[Launch the presentation](https://gitpitch.com/idia640/labs/master?p=lab2b#/) and check out the speaker notes below.

## Speaker notes

### Slide 1

- "Usability" is described as "appropriateness to a purpose" (John Brooke, 1986). (He seems to mean how well a tool works in the context for which it is used.
- SUS is designed to provide a single score; it is a  reliable, low cost usability test.
- Originally, the Likert-scale questions came from a pool of 50 potential questions and two different tasks; the results were examined statistically leading to the final 10 questions selected. Half the questions were framed positively and half negatively so that they respondants would have to make an effort to think about each statement.
- Calculation of scores is potentially error-prone:
  -- Positively worded items (odd questions) - scale position - 1
  -- Negatively worded items (even questions) - 5 - scale
  -- Total - multiply the sum of the item score by 2.5
- SUS ranges from 0-100 in 2.5 increments. This resultant curve matches well with an A-F grading with 68 as an average score.
- What were the prevailing alternatives to SUS? Effectiveness, efficiency, satisfaction, etc. But the metrics varied widely and it was difficult to make cross-system comparisons.

### Slide 2

- Sauro and Lewis note that the original assumption was reasonable... people might answer all yes or all no given repetitive positive/negative coding. But since the test is more complex, don't people make mistakes?
- And what about researchers making errors in scoring?
- Has this study been replicated? Sort of. 2008 UPA website study. There was an important difference from Brooke's original results; extreme intensity affected averages (e.g., people who passionately disagree with a negative question).
- We know that wording really matters. All positive questions leads a user to think you really want positive answers. So this is an important consideration.
- The experiment in Sauro & Lewis (2011) relied on recruitment from AMT - 4 websites 4 tasks, 20-30 subjects per site. 2 conditions (normal, positive only)
- Asynchronous. Could not control whether the website changed while the experiment was running (budget, bellco, sears, travelocity).
- Because of unexpected results in the first trial, the researchers designed a second; 213 subjects; 2 tasks, 7 sites; 15-17 subjects per site.
- The test is indeed "quick, but not dirty". You need a sample size of at least 12.
- Lewis & Sauro (2009) did a factor analysis and determined that questions 4 and 10 pertain to learnability vice usability. While the factor analysis was mentioned in this paper, they wrote a 2009 paper that goes into more detail.

### References

Brooke, J. (1996). SUS-A quick and dirty usability scale. Usability Evaluation in Industry, 189(194), 4-7.

Lewis, J. R., & Sauro, J. (2009, July). The factor structure of the system usability scale. In International Conference on Human Centered Design (pp. 94-103). Springer, Berlin, Heidelberg.

Sauro, J., & Lewis, J. R. (2011, May). When designing usability questionnaires, does it hurt to be positive? In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2215-2224). ACM.

Sauro, J. (Feb 2, 2011). Measuring usability with the system usability scale. Retrieved from: https://measuringu.com/sus/.
